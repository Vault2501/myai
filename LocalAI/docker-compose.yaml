networks:
  frontend:
    name: localai
    driver: bridge
    driver_opts:
      com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"

services:
  api:
    container_name: localai
    image: quay.io/go-skynet/local-ai:master-cublas-cuda12-ffmpeg
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 10m
      retries: 20
    networks:
      - frontend 
    ports:
      - 8080:8080
    environment:
      - DEBUG=true
      - MODELS_PATH=/models
      - SINGLE_ACTIVE_BACKEND=false
      - PYTHON_GRPC_MAX_WORKERS=2
      - LLAMACPP_PARALLEL=1
      - LOCALAI_PARALLEL_REQUESTS=true
      - PARALLEL_REQUESTS=true
      - WATCHDOG_IDLE=true
      - WATCHDOG_BUSY=true
      - WATCHDOG_IDLE_TIMEOUT=60m
      - WATCHDOG_BUSY_TIMEOUT=60m
    env_file:
      - .env
    volumes:
      - ../models/localai:/models:cached
      - ../images:/tmp/generated/images/
      - ./config:/config:cached
