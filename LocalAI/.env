## Set number of threads.
## Note: prefer the number of physical cores. Overbooking the CPU degrades performance notably.
THREADS=14

## Specify a different bind address (defaults to ":8080")
# ADDRESS=127.0.0.1:8080

## Define galleries.
## models will to install will be visible in `/models/available`
#GALLERIES=[{"url": "github:go-skynet/model-gallery/huggingface.yaml","name":"huggingface"}]
GALLERIES=[{"name":"model-gallery", "url":"github:go-skynet/model-gallery/index.yaml", "name":"model-gallery", "url":"https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/gallery/index.yaml"}]

#PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","name": "gpt4all-j"}]'
#PRELOAD_MODELS_CONFIG=/config/wizardlm-13b.yaml

## Default path for models
MODELS_PATH=../models

## Enable debug mode
DEBUG=true

## Disables COMPEL (Lets Stable Diffuser work)
COMPEL=0

## Enable/Disable single backend (useful if only one GPU is available)
#PYTHON_GRPC_MAX_WORKERS=2
#LLAMACPP_PARALLEL=1
#LOCALAI_PARALLEL_REQUESTS=true

## Specify a build type. Available: cublas, openblas, clblas.
BUILD_TYPE=cublas

## Uncomment and set to true to enable rebuilding from source
REBUILD=false
#CMAKE_ARGS="-DCMAKE_CUDA_ARCHITECTURES=50\;89"
# For Tesla M10
#CMAKE_ARGS="-DCMAKE_CUDA_ARCHITECTURES=50"
#CMAKE_BUILD_PARALLEL_LEVEL=14

## Enable go tags, available: stablediffusion, tts
## stablediffusion: image generation with stablediffusion
## tts: enables text-to-speech with go-piper 
## (requires REBUILD=true)
#
GO_TAGS=stablediffusion,tts

## Path where to store generated images
# IMAGE_PATH=/tmp

## Specify a default upload limit in MB (whisper)
# UPLOAD_LIMIT

# HUGGINGFACEHUB_API_TOKEN=Token here
#OPENAI_API_KEY=sk---
